{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "digits=load_digits()\n",
    "print(digits.keys())\n",
    "data=digits['data']\n",
    "images=digits['images']\n",
    "target=digits['target']\n",
    "target_names=digits['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_numbers(number1,number2):\n",
    "    \"\"\"Returns subset of digits sample data containing only the numbers passed to the function\n",
    "        \n",
    "       Takes number1 and number2 of the target_names as its arguments\n",
    "       Returns two arrays: data, target\n",
    "    \"\"\"\n",
    "    #if number1 and number2:\n",
    "    if True:\n",
    "        indices = np.where((digits['target'] == number1) | (digits['target'] == number2))\n",
    "        #print(indices)\n",
    "        data = (digits['data'])[indices]\n",
    "        target = (digits['target'])[indices]\n",
    "        #print(indices)\n",
    "    return data, target\n",
    "\n",
    "def reduced_dim(x):\n",
    "    res=np.empty([np.shape(x)[0],2])\n",
    "    for i in range(len(x)):\n",
    "        # Define the features here:\n",
    "        \n",
    "        res[i][0]=x[i][60]# Feature 1\n",
    "        res[i][1]=x[i][19]         # Feature 2\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes(features,labels, bincount=0):\n",
    "    \"\"\" calculates histograms for each of the D feature dimensions\n",
    "    \n",
    "        If calles with bincount = 0, calculates number of bins by Freedman-Diaconis\n",
    "        \n",
    "        returns\n",
    "        histograms - C x D x L array (C = #classes, D = #feature dimensions, L 0 #bins)\n",
    "        bincount - C x D x 2 array (last dimensions: [lower bound of bin, bin width] )\n",
    "    \n",
    "    \"\"\"\n",
    "    class0, class1 = features[np.where(labels==0)],features[np.where(labels==1)]\n",
    "   \n",
    "    if (bincount == 0):\n",
    "        ## suggested bin width by Freedman Diaconis\n",
    "        IQR_0 = iqr(class0,axis=0) \n",
    "        IQR_1 = iqr(class1,axis=0) \n",
    "\n",
    "        binwidth_0 = 2 * IQR_0 / (len(class0))**(1/3)\n",
    "        binwidth_1 = 2 * IQR_1 / (len(class1))**(1/3)\n",
    "\n",
    "        #calculate number of bins; exclude feature dimensions where IQR is 0 \n",
    "        bins_0 = np.ceil((np.max(class0[:,np.where(binwidth_0 != 0)],axis=0)-np.min(class0[:,np.where(binwidth_0 != 0)],axis=0))/binwidth_0[np.where(binwidth_0!=0)])\n",
    "        bins_0_ = np.zeros((features.shape[1]))\n",
    "        bins_0_[np.where(binwidth_0 != 0)] = bins_0\n",
    "        \n",
    "        bins_1 = np.ceil((np.max(class1[:,np.where(binwidth_1 != 0)],axis=0)-np.min(class1[:,np.where(binwidth_1 != 0)],axis=0))/binwidth_1[np.where(binwidth_1!=0)])\n",
    "        bins_1_ = np.zeros((features.shape[1]))\n",
    "        bins_1_[np.where(binwidth_1 != 0)] = bins_1\n",
    "\n",
    "        bincount = np.round(np.mean([bins_0,bins_1])) # set bin counts as averaged number of suggested bins (exclusive dimensions where IQR = 0)\n",
    "\n",
    "    binwidth0 = np.ceil((np.max(class0,axis=0)-np.min(class0,axis=0))/bincount)\n",
    "    binwidth1 = np.ceil((np.max(class1,axis=0)-np.min(class1,axis=0))/bincount)\n",
    "\n",
    "\n",
    "    histograms = np.zeros((2,features.shape[1],int(bincount)))\n",
    "    binning = np.ones((2,features.shape[1],2))\n",
    "        \n",
    "    binning[0,:,0] = np.min(class0,axis=0) # lower bound of first bin\n",
    "    binning[0,np.where(bins_0_ >= bincount),1] = binwidth0[np.where(bins_0_ >= bincount)] # bin width >= 1; check for\n",
    "                                                                                          # number of bins and if more than bincount, adjust bin width \n",
    "\n",
    "    binning[1,:,0] = np.min(class1,axis=0)\n",
    "    binning[1,np.where(bins_1_ >= bincount),1] = binwidth1[np.where(bins_1_ >= bincount)]\n",
    "    \n",
    "    ### fill bins\n",
    "    def frequency(k,i):\n",
    "        \"\"\"\n",
    "        Bins feature dimension i\"\"\"\n",
    "        class_ = class0 if k == 0 else class1\n",
    "       \n",
    "        unique, counts = np.unique(np.floor((class_[:,i] - binning[k,i,0])/binning[k,i,1]), return_counts=True)\n",
    "        return unique, counts\n",
    "    for k in [0,1]:\n",
    "        for i in range(features.shape[1]):\n",
    "            bin_, counts = frequency(k,i)\n",
    "            for m in range(len(bin_)):\n",
    "                histograms[k,i,int(m)] = counts[m]\n",
    "    return histograms, binning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 2 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-19cde3feaff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel39\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel39\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabel39\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel39\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_naive_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature39\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel39\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistograms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-185-030afa5bef9b>\u001b[0m in \u001b[0;36mfit_naive_bayes\u001b[0;34m(features, labels, bincount)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mbin_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mhistograms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11 is out of bounds for axis 2 with size 11"
     ]
    }
   ],
   "source": [
    "feature39, label39 = filter_numbers(3,9) \n",
    "label39[label39 == 3] = 0\n",
    "label39[label39 == 9] = 1\n",
    "histograms, binning = fit_naive_bayes(feature39, label39)\n",
    "print(histograms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program is failing when it comes to determining the counts of each individual bin. The problem is that the minimum bin width which is set to 1 for feature dimensions, where the suggested number of bins is less than the average, seems to be too low so that outliers count into bins of the full spectrum of the greyscale (up to 17 bins). As it is quite likely that the IQR and thus the recommended bin width after Freedman-Diaconis' rule is 0, we need to investigate a better way of treating outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
